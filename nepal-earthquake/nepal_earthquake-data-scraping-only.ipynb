{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer:** Scraping data is often considered illegal and unethical if done without proper permissions. This project and its content are intended **solely for educational purposes** to demonstrate technical concepts. Data is scraped from https://seismonepal.gov.np/earthquakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rMFBdgMvF7xN",
    "outputId": "edd34d96-f1d7-49ec-8caf-2e89481e50a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped page 1\n",
      "Scraped page 2\n",
      "Scraped page 3\n",
      "Scraped page 4\n",
      "Scraped page 5\n",
      "Scraped page 6\n",
      "Scraped page 7\n",
      "Scraped page 8\n",
      "Scraped page 9\n",
      "Scraped page 10\n",
      "Scraped page 11\n",
      "Scraped page 12\n",
      "Scraped page 13\n",
      "Scraped page 14\n",
      "Scraped page 15\n",
      "Scraped page 16\n",
      "Scraped page 17\n",
      "Scraped page 18\n",
      "Scraped page 19\n",
      "Scraped page 20\n",
      "Scraped page 21\n",
      "Scraped page 22\n",
      "Scraped page 23\n",
      "Scraped page 24\n",
      "Scraped page 25\n",
      "Scraped page 26\n",
      "Scraped page 27\n",
      "Scraped page 28\n",
      "Scraped page 29\n",
      "Scraped page 30\n",
      "Scraped page 31\n",
      "Scraped page 32\n",
      "Scraped page 33\n",
      "Scraped page 34\n",
      "Scraped page 35\n",
      "Scraped page 36\n",
      "Scraped page 37\n",
      "Scraped page 38\n",
      "Scraped page 39\n",
      "Scraped page 40\n",
      "Scraped page 41\n",
      "Scraped page 42\n",
      "Scraped page 43\n",
      "Scraped page 44\n",
      "Scraped page 45\n",
      "Scraped page 46\n",
      "Scraped page 47\n",
      "Scraped page 48\n",
      "Scraped page 49\n",
      "Scraped page 50\n",
      "Scraped page 51\n",
      "Scraped page 52\n",
      "Scraped page 53\n",
      "Scraped page 54\n",
      "Scraped page 55\n",
      "Scraped page 56\n",
      "Scraped page 57\n",
      "Scraped page 58\n",
      "Scraped page 59\n",
      "Scraped page 60\n",
      "Scraped page 61\n",
      "Scraped page 62\n",
      "data scraped successfully\n"
     ]
    }
   ],
   "source": [
    "#scraping data\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Scrape data from the website\n",
    "base_url = \"https://seismonepal.gov.np/earthquakes/index\"\n",
    "\n",
    "max_pages = 62  # Update as needed\n",
    "# max_pages = 5  # to test\n",
    "\n",
    "# Function to scrape data from a single page\n",
    "def scrape_page(page_url):\n",
    "    response = requests.get(page_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Locate the table\n",
    "    table = soup.find('table', {'class': 'table table-striped table-bordered'})\n",
    "    rows = table.find('tbody').find_all('tr')\n",
    "\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        cols = [col.text.strip() for col in cols]\n",
    "        if cols:  # Check for non-empty rows\n",
    "            data.append({\n",
    "                'Date': cols[1],\n",
    "                'Time': cols[2],\n",
    "                'Latitude': str(cols[3]),\n",
    "                'Longitude': str(cols[4]),\n",
    "                'Magnitude': cols[5],\n",
    "                'Epicenter': cols[6]\n",
    "            })\n",
    "    return data\n",
    "\n",
    "# Main function to scrape multiple pages\n",
    "def scrape_all_pages(base_url, max_pages):\n",
    "    all_data = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{base_url}?page={page}\"\n",
    "        page_data = scrape_page(url)\n",
    "        all_data.extend(page_data)\n",
    "        print(f\"Scraped page {page}\") #this step can take some time, to see the progress of scraping data, uncomment this line\n",
    "    return all_data\n",
    "\n",
    "data = scrape_all_pages(base_url, max_pages)\n",
    "# print(data)\n",
    "print('data scraped successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to uncleaned_earthquake_data_nepal.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "# Save to CSV\n",
    "df.to_csv('uncleaned_earthquake_data_nepal.csv', index=False)\n",
    "print(\"Data saved to uncleaned_earthquake_data_nepal.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1237 entries, 0 to 1236\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Date       1237 non-null   object\n",
      " 1   Time       1237 non-null   object\n",
      " 2   Latitude   1237 non-null   object\n",
      " 3   Longitude  1237 non-null   object\n",
      " 4   Magnitude  1237 non-null   object\n",
      " 5   Epicenter  1237 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 58.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Date                    Time Latitude  \\\n",
      "count                               1237                    1237     1237   \n",
      "unique                               824                     981      310   \n",
      "top     B.S.: 2072-01-12A.D.: 2015-04-25  Local: 00:24UTC: 18:39    27.79   \n",
      "freq                                  40                       4       25   \n",
      "\n",
      "       Longitude Magnitude Epicenter  \n",
      "count       1237      1237      1237  \n",
      "unique       518        30       133  \n",
      "top        86.07       4.0   Dolakha  \n",
      "freq          11       463       188  \n"
     ]
    }
   ],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Date                    Time Latitude  \\\n",
      "0  B.S.: 2081-09-23A.D.: 2025-01-07  Local: 06:50UTC: 01:05    28.31   \n",
      "1  B.S.: 2081-09-19A.D.: 2025-01-03  Local: 08:03UTC: 02:18    29.57   \n",
      "2  B.S.: 2081-09-19A.D.: 2025-01-03  Local: 08:29UTC: 02:44    28.54   \n",
      "3  B.S.: 2081-09-18A.D.: 2025-01-02  Local: 13:02UTC: 07:17    27.77   \n",
      "4  B.S.: 2081-09-16A.D.: 2024-12-31  Local: 07:54UTC: 02:09    29.45   \n",
      "\n",
      "  Longitude Magnitude        Epicenter  \n",
      "0     87.37       7.0  Dinggye, China*  \n",
      "1     82.19       4.4             Mugu  \n",
      "2     84.13       4.1           Manang  \n",
      "3     85.57       4.8    Sindhupalchok  \n",
      "4     80.86       4.6          Baitadi  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
