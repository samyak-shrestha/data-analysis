{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer:** Scraping data is often considered illegal and unethical if done without proper permissions. This project and its content are intended **solely for educational purposes** to demonstrate technical concepts. Data is scraped from https://seismonepal.gov.np/earthquakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rMFBdgMvF7xN",
    "outputId": "edd34d96-f1d7-49ec-8caf-2e89481e50a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data scraped updated\n"
     ]
    }
   ],
   "source": [
    "#scraping data\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Scrape data from the website\n",
    "base_url = \"https://seismonepal.gov.np/earthquakes/index\"\n",
    "\n",
    "max_pages = 63  # Update as needed\n",
    "\n",
    "# Function to scrape data from a single page\n",
    "def scrape_page(page_url):\n",
    "    response = requests.get(page_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Locate the table\n",
    "    table = soup.find('table', {'class': 'table table-striped table-bordered'})\n",
    "    rows = table.find('tbody').find_all('tr')\n",
    "\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        cols = [col.text.strip() for col in cols]\n",
    "        if cols:  # Check for non-empty rows\n",
    "            data.append({\n",
    "                'Date': cols[1],\n",
    "                'Time': cols[2],\n",
    "                'Latitude': str(cols[3]),\n",
    "                'Longitude': str(cols[4]),\n",
    "                'Magnitude': cols[5],\n",
    "                'Epicenter': cols[6]\n",
    "            })\n",
    "    return data\n",
    "\n",
    "# Main function to scrape multiple pages\n",
    "def scrape_all_pages(base_url, max_pages):\n",
    "    all_data = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{base_url}?page={page}\"\n",
    "        page_data = scrape_page(url)\n",
    "        all_data.extend(page_data)\n",
    "        # print(f\"Scraped page {page}\") #this step can take some time, to see the progress of scraping data, uncomment this line\n",
    "    return all_data\n",
    "\n",
    "datas = scrape_all_pages(base_url, max_pages)\n",
    "# print(data)\n",
    "print('data scraped updated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data saved to earthquake_data_nepal.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Cleaning Functions ---\n",
    "\n",
    "def clean_date(date_str):\n",
    "    if 'A.D.:' in date_str:\n",
    "        return date_str.split('A.D.:')[1].strip()\n",
    "    return None\n",
    "\n",
    "def clean_time(time_str):\n",
    "    if 'Local:' in time_str:\n",
    "        return time_str.split('Local:')[1].split('UTC:')[0].strip()\n",
    "    return None\n",
    "\n",
    "def clean_numeric(value):\n",
    "    try:\n",
    "        return float(str(value).replace(':', '.'))\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def clean_epicenter(epicenter_str):\n",
    "    # Remove asterisk and extra spaces\n",
    "    return epicenter_str.replace('*', '').strip()\n",
    "\n",
    "# --- Process Data ---\n",
    "\n",
    "cleaned_data = []\n",
    "for entry in datas:\n",
    "    cleaned_entry = {\n",
    "        'Date': clean_date(entry['Date']),\n",
    "        'Time': clean_time(entry['Time']),\n",
    "        'Latitude': clean_numeric(entry['Latitude']),\n",
    "        'Longitude': clean_numeric(entry['Longitude']),\n",
    "        'Magnitude': clean_numeric(entry['Magnitude']),\n",
    "        'Epicenter': clean_epicenter(entry['Epicenter'])\n",
    "    }\n",
    "    cleaned_data.append(cleaned_entry)\n",
    "\n",
    "# --- Convert to DataFrame ---\n",
    "df = pd.DataFrame(cleaned_data)\n",
    "\n",
    "# Drop rows with missing coordinates\n",
    "df = df.dropna(subset=['Latitude', 'Longitude'], how='any')\n",
    "\n",
    "# Convert 'Date' to datetime.date\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date\n",
    "\n",
    "# Combine 'Date' and 'Time' into full datetime\n",
    "df['Datetime'] = pd.to_datetime(df['Date'].astype(str) + ' ' + df['Time'], errors='coerce')\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('data/earthquake_data_nepal.csv', index=False)\n",
    "print(\"✅ Data saved to earthquake_data_nepal.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date   Time  Latitude  Longitude  Magnitude      Epicenter  \\\n",
      "0     2025-04-04  20:10     28.96      82.12        5.5      Jajarkot*   \n",
      "1     2025-04-04  20:07     28.95      82.12        5.2       Jajarkot   \n",
      "2     2025-04-03  17:04     30.02      80.84        4.0       Darchula   \n",
      "3     2025-03-26  19:44     29.69      81.82        4.5          Humla   \n",
      "4     2025-03-26  18:27     28.70      86.74        5.5  Tingri, China   \n",
      "...          ...    ...       ...        ...        ...            ...   \n",
      "1232  1995-01-29  02:37     26.85      86.11        4.5        Dhanusa   \n",
      "1233  1995-01-27  23:05     29.08      81.73        4.3        Kalikot   \n",
      "1234  1995-01-22  11:58     27.90      87.80        4.0      Taplejung   \n",
      "1235  1995-01-19  12:18     28.35      83.44        4.3         Myagdi   \n",
      "1236  1995-01-05  17:23     29.82      80.95        4.4       Darchula   \n",
      "\n",
      "                 Datetime  \n",
      "0     2025-04-04 20:10:00  \n",
      "1     2025-04-04 20:07:00  \n",
      "2     2025-04-03 17:04:00  \n",
      "3     2025-03-26 19:44:00  \n",
      "4     2025-03-26 18:27:00  \n",
      "...                   ...  \n",
      "1232  1995-01-29 02:37:00  \n",
      "1233  1995-01-27 23:05:00  \n",
      "1234  1995-01-22 11:58:00  \n",
      "1235  1995-01-19 12:18:00  \n",
      "1236  1995-01-05 17:23:00  \n",
      "\n",
      "[1237 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display DataFrame\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
